\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[margin=2.5cm]{geometry}
\usepackage{setspace}
\usepackage{titlesec}
\usepackage{lmodern}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{array}
\usepackage{caption}
\usepackage{longtable}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=black,
    urlcolor=blue
}
\setstretch{1.3}

\title{\textbf{Results and Discussion}\\
\large Validation of an AI Tutor for Autonomous Learning – First Experimental Iteration}
\author{AI4PROSA Project – University of Alicante}
\date{}

\begin{document}
\maketitle

\section{Overview}

This section presents the results of the first experimental iteration conducted to validate the effectiveness and pedagogical viability of an AI-based tutoring system developed by the AI4PROSA project at the University of Alicante. The analysis integrates quantitative and qualitative data obtained from five students and one instructor who participated in a two-hour learning session on the lesson \emph{``Obligations''} within the module on Child Product Safety.

The results are organized according to the evaluation criteria defined in the experimental design: (1) Learning effectiveness, (2) Personalization and adaptation, (3) Autonomy and reduction of teacher dependence, (4) Usability and pedagogical quality, and (5) Overall satisfaction and feasibility.

The design’s pre-established success thresholds were:
\begin{itemize}
    \item \textbf{Learning effectiveness:} average score $\geq$ 7/10 on the content test.
    \item \textbf{Perceived experience:} average rating $\geq$ 3.5/5 on student and teacher questionnaires.
    \item \textbf{Pedagogical viability:} teacher rating $\geq$ 4/5.
\end{itemize}

\section{Learning Effectiveness}

\subsection{Quantitative outcomes}

The post-session content test yielded a mean score of 7/10 (range 2–7), meeting the success criterion for learning effectiveness. Four of five students achieved or surpassed the target score, while one student obtained 2/10, creating a bimodal distribution.

High-performing items included basic regulatory concepts, such as identifying the manufacturer as an economic operator and recognizing consumer remedies during product recalls (both 100\% correct). Conversely, items requiring interpretive understanding—such as reporting channels or documentation retention periods—showed accuracy rates of only 20–40\%.

\subsection{Interpretation}

These results indicate that the AI tutor was effective in supporting factual and procedural learning, fostering accurate understanding of core concepts. However, lower performance on interpretive items suggests that explanations were occasionally too general or lacked contextual specificity.

This interpretation aligns with the qualitative interview data, where students reported that the exam included aspects not fully covered by the tutor and that some responses were too general.

\subsection{Discussion}

The AI tutor proved effective at promoting attention, recall, and conceptual clarity in foundational content. Nonetheless, it requires refinement to improve the depth and precision of its legal and procedural explanations, ensuring closer alignment between instructional content and assessment questions.

\section{Student Experience and Perceived Impact}

\subsection{Personalization and Adaptation}

Across the personalization items, students rated the tutor highly (mean = 4.4/5). They perceived it as responsive and flexible, especially in providing additional examples when needed (P2 = 4.8). Some learners, however, described the interaction as ``not entirely individualized,'' indicating reactive rather than proactive adaptation.

Overall, content adaptability was evident and appreciated, but future iterations should introduce an initial diagnostic dialogue to calibrate the content level automatically.

\subsection{Autonomy and Reduction of Teacher Dependence}

This dimension yielded the highest ratings (mean = 4.93/5). Students unanimously agreed that they could resolve doubts independently and work at their own pace. The teacher corroborated these perceptions, confirming no academic interventions during the session and no students requiring individual help.

The data validate the hypothesis that the AI tutor significantly reduces the teacher’s workload while supporting autonomous learning.

\subsection{Usability and Learning Experience}

Usability was rated very high (mean = 4.7/5). Students found the tutor intuitive and clear to use, with 80\% strongly agreeing that explanations aided comprehension. Qualitative data also reflected high engagement and sustained attention, with learners emphasizing that they felt more focused than in traditional lectures.

\subsection{Satisfaction and Continuity}

Students expressed high overall satisfaction (mean = 4.2/5) and willingness to continue using the AI tutor (P10 = 4.4). However, 80\% preferred a hybrid model combining AI tutoring with teacher-led instruction, confirming the system’s role as a complementary rather than substitutive tool. This aligns with the experimental principle that the tutor reduces, but does not replace, the teacher’s role.

\section{Teacher Evaluation}

\subsection{Workload Reduction}

The teacher’s questionnaire yielded an average score of 4.7/5 in the workload dimension. Only one to three academic interventions were required during the session, and no student needed direct assistance. These results demonstrate that the AI tutor substantially reduces instructor workload and supports self-directed learning.

\subsection{Observed Personalization}

The instructor rated observed adaptation positively (mean = 4.3/5), noting that the tutor provided differentiated responses and allowed advanced students to progress independently. This observation reinforces the system’s capacity to respond dynamically to learner inputs.

\subsection{Pedagogical Quality}

Pedagogical quality achieved the maximum score (5/5). The instructor found all explanations accurate, coherent, and pedagogically appropriate, and did not need to correct or extend any tutor responses. This confirms the reliability of the tutor’s content and its instructional design.

\section{Integrated Discussion}

\subsection{Convergence of Findings}

Across all data sources, four consistent patterns emerged:
\begin{enumerate}
    \item Learning effectiveness met the target criterion (mean = 7/10).
    \item Personalization and usability were rated highly (mean $\approx$ 4.5/5).
    \item Autonomy and workload reduction achieved outstanding scores (mean $\geq$ 4.9/5).
    \item Pedagogical quality was confirmed by both students and the instructor.
\end{enumerate}

The triangulation of quantitative data and qualitative insights confirms that the AI tutor is pedagogically viable and capable of sustaining meaningful learning interactions with minimal teacher intervention.

\subsection{Interpretation and Implications}

The results directly address the central research question: \emph{Can we trust an AI system to tutor students effectively, reducing teacher workload while increasing learning personalization?}

Evidence from this pilot study suggests a positive answer. The AI tutor can manage instructional interactions autonomously, maintain engagement, and deliver pedagogically sound explanations. However, its current personalization remains responsive rather than diagnostic, and its explanatory depth can be further enhanced for more complex topics.

\section{Recommendations for Future Iterations}

\begin{enumerate}
    \item Integrate a diagnostic pre-assessment to calibrate content difficulty and personalize learning pathways.
    \item Expand the knowledge base with more contextual examples and procedural explanations.
    \item Align test items more precisely with tutor content to ensure content validity.
    \item Develop a teacher monitoring dashboard for real-time observation and progress tracking.
    \item Extend the experiment longitudinally to assess retention and long-term motivation.
\end{enumerate}

\section{Conclusion}

The first experimental iteration demonstrates that the AI tutor developed by AI4PROSA meets or exceeds all predefined success criteria in learning effectiveness, personalization, usability, and pedagogical quality. Both students and the instructor perceive it as a credible, adaptive, and effective educational tool that enhances autonomy and reduces teacher workload.

While further refinement is needed to deepen personalization and conceptual precision, these results provide strong empirical evidence that the AI tutor constitutes a viable prototype for semi-autonomous learning support in higher education.

\section{Results and Discussion}

This section presents and discusses the results obtained during the first pilot iteration of the experimental design. The analysis is strictly based on the predefined instruments, success criteria, and empirical data collected during the intervention. No external sources are considered, in accordance with the exploratory and design-oriented nature of this study.

\subsection{Learning Effectiveness}

Learning effectiveness was assessed through a post-intervention content knowledge test composed of ten multiple-choice questions aligned with the learning objectives of Lesson 2 (``Obligations'' in the European framework for child product safety). The test was administered individually, without assistance from either the AI tutor or the instructor.

All five students achieved high scores, ranging from 8/10 to 9/10, with a mean score of 8.8/10 (88\%). All participants exceeded the predefined success threshold of 7/10 established in the experimental design as the minimum criterion for effective learning.

Self-reported prior knowledge was very low across participants, with three students indicating ``very little'' prior knowledge and two indicating ``no prior knowledge at all.'' Although no formal pre-test was conducted, the combination of low initial familiarity and high post-test performance provides strong preliminary evidence that the AI tutor enabled effective acquisition of the target content.

Accordingly, the learning effectiveness criterion defined for this pilot iteration was clearly fulfilled.

\subsection{Student Questionnaire Results}

Student perceptions were measured using a quantitative questionnaire comprising ten Likert-scale items grouped into four dimensions: perceived personalization, learner autonomy, usability, and satisfaction.

\subsubsection{Perceived Personalization}

Perceived personalization (items P1--P3) yielded a mean score of approximately 4.2/5, exceeding the predefined success threshold of 3.5. Students reported that the tutor responded to their individual needs and frequently provided additional explanations or alternative examples.

However, the item related to adaptation to learning pace showed slightly lower values, indicating that personalization was not experienced uniformly by all students. This suggests that personalization was primarily reactive rather than fully adaptive at the pedagogical level.

\subsubsection{Learner Autonomy and Reduction of Teacher Dependence}

Items related to learner autonomy (P4--P6) produced the strongest results, with a mean score of approximately 4.7/5. Students reported that they were able to resolve doubts without teacher intervention and felt capable of learning autonomously with the tutor.

These results strongly support the central hypothesis of the experimental design, namely that the AI tutor can foster autonomous learning while reducing dependence on the instructor during the learning session.

\subsubsection{Usability and User Experience}

Usability and clarity of explanations (P7--P8) were rated highly, with a mean score of approximately 4.4/5. These results indicate that interaction with the tutor did not pose a usability barrier and that students were able to engage with the system effectively from the outset.

\subsubsection{Satisfaction and Continuity Intention}

Satisfaction-related items (P9--P10) showed positive but more moderate values, with a mean score of approximately 3.8/5. While students generally expressed willingness to continue using the tutor, preference over traditional instruction was not unanimous.

This result suggests that students perceive the tutor primarily as a valuable complement to traditional teaching rather than as a complete replacement.

\subsection{Teacher Questionnaire Results}

The teacher questionnaire provided an observational perspective on workload reduction, personalization, pedagogical quality, and classroom viability.

\subsubsection{Workload Reduction}

During the two-hour session, the teacher reported zero academic interventions and indicated that no student required individual academic support. Compared to a traditional session on the same topic, the perceived workload was rated as 4/5, indicating a clear reduction.

These findings confirm that the tutor effectively reduced instructional workload while allowing students to progress autonomously.

\subsubsection{Observed Personalization}

Observed personalization (items P4--P6) received very high ratings, with an average score of approximately 4.7/5. The teacher reported clear differentiation between students and noted that advanced learners were able to progress without waiting for the rest of the group.

This observation reinforces the student-reported results and confirms that personalization was not only perceived subjectively but also observable at the classroom level.

\subsubsection{Pedagogical Quality and Viability}

Pedagogical quality was rated as acceptable but not outstanding (3/5), while no correction or supplementation of the tutor's explanations was required (5/5). This pattern indicates that explanations were factually correct and pedagogically sound, though limited in instructional richness.

Viability for regular use was rated positively (4/5), and the tutor was strongly recommended to other instructors (5/5), suggesting high practical value in real teaching contexts.

\subsection{Integrated Discussion}

Across all instruments, results converge toward a consistent pattern. The AI tutor demonstrated high learning effectiveness, strong support for learner autonomy, and substantial reduction of teacher workload, fulfilling all success criteria defined for this pilot iteration.

A key distinction emerges between cognitive outcomes and pedagogical experience. While learning outcomes were strong and homogeneous, both student and teacher data indicate that pedagogical depth and adaptive explanation strategies represent the main areas for improvement.

The moderate preference over traditional instruction does not contradict the positive effectiveness results. Rather, it suggests that the tutor is currently best positioned as a complementary tool within a blended instructional model.

\subsection{Limitations and Implications for Future Iterations}

As defined in the experimental design, this study is limited by a small sample size, the absence of a control group, and the lack of a formal pre-test. Consequently, results cannot be generalized statistically.

Nevertheless, within the scope of a pilot validation focused on feasibility and design refinement, the results provide clear justification for progression to a second iteration involving larger samples, comparative conditions, and enhanced pedagogical adaptivity.

\subsection{Conclusion}

The results of this pilot study indicate that the AI tutor is educationally effective, operationally viable, and pedagogically promising. All success criteria defined for the first iteration were met, supporting advancement to a more rigorous experimental phase.



\end{document}


